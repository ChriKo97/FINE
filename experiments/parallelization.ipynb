{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "import FINE.spagat.utils as spu\n",
    "import FINE.spagat.RE_representation_utils as RE_rep_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'C:/Users/s.patil/Documents/code/fine/examples/SPAGAT/RE_representation/InputData/'\n",
    "shapefile = os.path.join(DATA_PATH, 'Shapefiles/CZE.shp')\n",
    "GRIDDED_WIND_DATA_PATH = os.path.join(DATA_PATH, 'CZE_wind.nc4') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@spu.timer\n",
    "def represent_RE_technology(gridded_RE_ds, \n",
    "                            CRS_attr,\n",
    "                            shp_file,\n",
    "                            n_timeSeries_perRegion=1,  \n",
    "                            capacity_var_name='capacity',\n",
    "                            capfac_var_name='capacity factor',\n",
    "                            longitude='x', \n",
    "                            latitude='y',\n",
    "                            time='time',\n",
    "                            index_col='region_ids', \n",
    "                            geometry_col='geometry',\n",
    "                            linkage='average'):\n",
    "\n",
    "    def _simply_aggregate_RE_technology(region):\n",
    "    \n",
    "        #STEP 1. Create resultant xarray dataset \n",
    "        time_steps = rasterized_RE_ds[time].values  \n",
    "        n_timeSteps = len(time_steps)\n",
    "\n",
    "        ## time series \n",
    "        data = np.zeros((n_timeSteps, 1))\n",
    "\n",
    "        represented_timeSeries = xr.DataArray(data, [(time, time_steps),\n",
    "                                                    ('region_ids', [region])])\n",
    "\n",
    "        #capacities\n",
    "        represented_capacities = xr.DataArray(0, [('region_ids', [region])])\n",
    "\n",
    "        #STEP 2. Representation\n",
    "        regional_ds = rasterized_RE_ds.sel(region_ids = region)\n",
    "    \n",
    "        regional_capfac_da = regional_ds[capfac_var_name].where(regional_ds.rasters == 1)\n",
    "        regional_capacity_da = regional_ds[capacity_var_name].where(regional_ds.rasters == 1)\n",
    "\n",
    "        #STEP 2b. Preprocess regional capfac and capacity dataArrays \n",
    "\n",
    "        #STEP 2b (i). Restructure data\n",
    "        regional_capfac_da = regional_capfac_da.stack(x_y = [longitude, latitude]) \n",
    "        regional_capfac_da = regional_capfac_da.transpose(transpose_coords= True) \n",
    "\n",
    "        regional_capacity_da = regional_capacity_da.stack(x_y = [longitude, latitude])\n",
    "        regional_capacity_da = regional_capacity_da.transpose(transpose_coords= True)\n",
    "\n",
    "        #STEP 2b (ii). Remove all time series with 0 values \n",
    "        regional_capfac_da = regional_capfac_da.where(regional_capacity_da > 0)\n",
    "        regional_capacity_da = regional_capacity_da.where(regional_capacity_da > 0)\n",
    "\n",
    "        #STEP 2b (iii). Drop NAs \n",
    "        regional_capfac_da = regional_capfac_da.dropna(dim='x_y')\n",
    "        regional_capacity_da = regional_capacity_da.dropna(dim='x_y')\n",
    "\n",
    "        #Print out number of time series in the region \n",
    "        n_ts = len(regional_capfac_da['x_y'].values)\n",
    "        print(f'Number of time series in {region}: {n_ts}')\n",
    "\n",
    "        #STEP 2c. Get power curves from capacity factor time series and capacities \n",
    "        regional_power_da = regional_capacity_da * regional_capfac_da\n",
    "\n",
    "        #STEP 2d. Aggregation\n",
    "        ## capacity\n",
    "        capacity_total = regional_capacity_da.sum(dim = 'x_y').values\n",
    "        represented_capacities.loc[region] = capacity_total\n",
    "        \n",
    "        ## capacity factor \n",
    "        power_total = regional_power_da.sum(dim = 'x_y').values\n",
    "        capfac_total = power_total/capacity_total\n",
    "        \n",
    "        represented_timeSeries.loc[:,region] = capfac_total\n",
    "\n",
    "        #STEP 3. Create resulting dataset \n",
    "        regional_represented_RE_ds = xr.Dataset({capacity_var_name: represented_capacities,\n",
    "                                        capfac_var_name: represented_timeSeries}) \n",
    "\n",
    "    \n",
    "        return regional_represented_RE_ds \n",
    "\n",
    "\n",
    "    def _cluster_RE_technology(region):\n",
    "        start_time = timeit\n",
    "        \n",
    "        #STEP 1. Create resultant xarray dataset \n",
    "        time_steps = rasterized_RE_ds[time].values  \n",
    "        n_timeSteps = len(time_steps)\n",
    "\n",
    "        TS_ids = [f'TS_{i}' for i in range(n_timeSeries_perRegion)] \n",
    "\n",
    "        ## time series \n",
    "        data = np.zeros((n_timeSteps, 1, n_timeSeries_perRegion))\n",
    "\n",
    "        represented_timeSeries = xr.DataArray(data, [(time, time_steps),\n",
    "                                                    ('region_ids', [region]),\n",
    "                                                    ('TS_ids', TS_ids)])\n",
    "\n",
    "        data = np.zeros((1, n_timeSeries_perRegion))\n",
    "\n",
    "        #capacities\n",
    "        represented_capacities = xr.DataArray(data, [('region_ids', [region]),\n",
    "                                                    ('TS_ids', TS_ids)])\n",
    "\n",
    "        #STEP 2. Representation\n",
    "        regional_ds = rasterized_RE_ds.sel(region_ids = region)\n",
    "    \n",
    "        regional_capfac_da = regional_ds[capfac_var_name].where(regional_ds.rasters == 1)\n",
    "        regional_capacity_da = regional_ds[capacity_var_name].where(regional_ds.rasters == 1)\n",
    "\n",
    "        #STEP 2b. Preprocess regional capfac and capacity dataArrays \n",
    "\n",
    "        #STEP 2b (i). Restructure data\n",
    "        regional_capfac_da = regional_capfac_da.stack(x_y = [longitude, latitude]) \n",
    "        regional_capfac_da = regional_capfac_da.transpose(transpose_coords= True) \n",
    "\n",
    "        regional_capacity_da = regional_capacity_da.stack(x_y = [longitude, latitude])\n",
    "        regional_capacity_da = regional_capacity_da.transpose(transpose_coords= True)\n",
    "\n",
    "        #STEP 2b (ii). Remove all time series with 0 values \n",
    "        regional_capfac_da = regional_capfac_da.where(regional_capacity_da > 0)\n",
    "        regional_capacity_da = regional_capacity_da.where(regional_capacity_da > 0)\n",
    "\n",
    "        #STEP 2b (iii). Drop NAs \n",
    "        regional_capfac_da = regional_capfac_da.dropna(dim='x_y')\n",
    "        regional_capacity_da = regional_capacity_da.dropna(dim='x_y')\n",
    "\n",
    "        #Print out number of time series in the region \n",
    "        n_ts = len(regional_capfac_da['x_y'].values)\n",
    "        print(f'Number of time series in {region}: {n_ts}')\n",
    "\n",
    "        #STEP 2c. Get power curves from capacity factor time series and capacities \n",
    "        regional_power_da = regional_capacity_da * regional_capfac_da\n",
    "        \n",
    "        end_time = timeit\n",
    "        \n",
    "        print(f'time taken for preprocessing: {end_time - start_time}')\n",
    "        \n",
    "        start_time = timeit\n",
    "        \n",
    "        #STEP 2d. Clustering  \n",
    "        agg_cluster = AgglomerativeClustering(n_clusters=n_timeSeries_perRegion, \n",
    "                                              affinity=\"euclidean\",  \n",
    "                                              linkage=linkage)\n",
    "        agglomerative_model = agg_cluster.fit(regional_capfac_da)\n",
    "        \n",
    "        end_time = timeit\n",
    "        \n",
    "        print(f'time taken for clustering: {end_time - start_time}')\n",
    "        \n",
    "        start_time = timeit\n",
    "        #STEP 2e. Aggregation\n",
    "        for i in range(np.unique(agglomerative_model.labels_).shape[0]):\n",
    "            ## Aggregate capacities \n",
    "            cluster_capacity = regional_capacity_da[agglomerative_model.labels_ == i]\n",
    "            cluster_capacity_total = cluster_capacity.sum(dim = 'x_y').values\n",
    "\n",
    "            represented_capacities.loc[region, TS_ids[i]] = cluster_capacity_total\n",
    "\n",
    "            #aggregate capacity factor \n",
    "            cluster_power = regional_power_da[agglomerative_model.labels_ == i]\n",
    "            cluster_power_total = cluster_power.sum(dim = 'x_y').values\n",
    "            cluster_capfac_total = cluster_power_total/cluster_capacity_total\n",
    "\n",
    "            represented_timeSeries.loc[:,region, TS_ids[i]] = cluster_capfac_total\n",
    "            \n",
    "        #STEP 3. Create resulting dataset \n",
    "        regional_represented_RE_ds = xr.Dataset({capacity_var_name: represented_capacities,\n",
    "                                        capfac_var_name: represented_timeSeries})  \n",
    "        \n",
    "        end_time = timeit\n",
    "        \n",
    "        print(f'time taken for aggregation: {end_time - start_time}')\n",
    "        \n",
    "        return regional_represented_RE_ds \n",
    "\n",
    "\n",
    "    #STEP 1. Rasterize the gridded dataset\n",
    "    rasterized_RE_ds = RE_rep_utils.rasterize_xr_ds(gridded_RE_ds, \n",
    "                                                    CRS_attr,\n",
    "                                                    shp_file, \n",
    "                                                    index_col, \n",
    "                                                    geometry_col,\n",
    "                                                    longitude, \n",
    "                                                    latitude)\n",
    "\n",
    "\n",
    "    region_ids = rasterized_RE_ds['region_ids'].values \n",
    "\n",
    "\n",
    "    \n",
    "    results = []\n",
    "    if n_timeSeries_perRegion==1:\n",
    "        for region in region_ids:\n",
    "            results.append(_simply_aggregate_RE_technology(region))\n",
    "    else:\n",
    "        for region in region_ids:\n",
    "            results.append(_cluster_RE_technology(region))\n",
    "\n",
    "    represented_RE_ds =  xr.merge(results)\n",
    "   \n",
    "    return represented_RE_ds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time series in 39_cz: 364\n",
      "time taken for preprocessing: 0.0\n",
      "time taken for clustering: 0.0\n",
      "time taken for aggregation: 0.0\n",
      "Number of time series in 40_cz: 213\n",
      "time taken for preprocessing: 0.0\n",
      "time taken for clustering: 0.0\n",
      "time taken for aggregation: 0.0\n",
      "elapsed time for represent_RE_technology: 0.05 minutes\n"
     ]
    }
   ],
   "source": [
    "represented_re_ds = represent_RE_technology(gridded_RE_ds = GRIDDED_WIND_DATA_PATH,\n",
    "                                            CRS_attr = 'xy_reference_system',\n",
    "                                            shp_file = shapefile,\n",
    "                                            n_timeSeries_perRegion = 5,\n",
    "                                            capacity_var_name='capacity',\n",
    "                                            capfac_var_name='capfac',\n",
    "                                            longitude='x', \n",
    "                                            latitude='y',\n",
    "                                            time='time',\n",
    "                                            index_col='e-id', \n",
    "                                            geometry_col='geometry',\n",
    "                                            linkage='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anomaly(da, groupby_type=\"time.month\"):\n",
    "    gb = da.groupby(groupby_type)\n",
    "    clim = gb.mean(dim=\"time\")\n",
    "    return clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n",
    "month = xr.DataArray(time.month, coords={\"time\": time}, dims=[\"time\"])\n",
    "np.random.seed(123)\n",
    "array = xr.DataArray(\n",
    "    np.random.rand(len(time)),\n",
    "     dims=[\"time\"],\n",
    "     coords={\"time\": time, \"month\": month},).chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = calculate_anomaly(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array.map_blocks(calculate_anomaly, template=new_array).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
