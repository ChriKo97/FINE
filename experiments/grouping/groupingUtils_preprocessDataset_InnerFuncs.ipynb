{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import warnings\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn import metrics\n",
    "\n",
    "import FINE.spagat.dataset as spd\n",
    "from ipynb.fs.full import grouping_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test dataset - (test_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = ['01_reg','02_reg','03_reg']\n",
    "TimeStep = ['T0','T1']\n",
    "space_2 = space.copy()\n",
    "component = ['c1','c2','c3','c4']\n",
    "Period = [0]\n",
    "\n",
    "demand = np.stack([[[[np.nan,np.nan, np.nan] for i in range(2)]],\n",
    "                        [[[1, 0.9,  2],\n",
    "                          [1, 0,  0.9]]],\n",
    "                        [[[np.nan,np.nan, np.nan] for i in range(2)]],\n",
    "                        [[[0,   1, 1],\n",
    "                          [0.3, 2, 1]]]])\n",
    "demand = xr.DataArray(demand, coords=[component, Period, TimeStep, space], dims=['component', 'Period', 'TimeStep','space'])\n",
    "cap_1d = np.stack([[0.9,  1,  0.9],\n",
    "                        [0,    0,  0],\n",
    "                        [0.9,  1,  0.9],\n",
    "                        [np.nan] *3])\n",
    "cap_1d = xr.DataArray(cap_1d, coords=[component,space], dims=['component','space'])\n",
    "dist_2d = np.stack([[[0,1,2],[1,0,10],[2,10,0]],\n",
    "                         [[0,0.1,0.2],[0.1,0,1],[0.2,1,0]],\n",
    "                         [[np.nan] * 3 for i in range(3)],\n",
    "                         [[np.nan] * 3 for i in range(3)]])\n",
    "dist_2d = xr.DataArray(dist_2d, coords=[component,space,space_2], dims=['component','space','space_2'])\n",
    "\n",
    "ds = xr.Dataset({'operationFixRate': demand, '1d_capacity': cap_1d, '2d_distance': dist_2d})\n",
    "\n",
    "sds = spd.SpagatDataset()\n",
    "sds.xr_dataset = ds\n",
    "\n",
    "sds.xr_dataset\n",
    "dataset = sds.xr_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data that comes from preprocessDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ts = {}\n",
    "vars_1d = {}\n",
    "vars_2d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for varname, da in dataset.data_vars.items():\n",
    "        # sort the dimensions\n",
    "        if sorted(da.dims) == sorted(('component','Period','TimeStep', 'space')):   #TODO: maybe space should be generalized with additional variable - dimension_description ?\n",
    "            # Period is not considered -> TODO: consider the Period dimension.\n",
    "            da = da.transpose('Period','component','space','TimeStep')[0]  \n",
    "            vars_ts[varname] = da\n",
    "\n",
    "        elif sorted(da.dims) == sorted(('component','space')):\n",
    "            vars_1d[varname] = da\n",
    "\n",
    "        elif sorted(da.dims) == sorted(('component','space','space_2')):\n",
    "            vars_2d[varname] = da\n",
    "\n",
    "        else:\n",
    "            warnings.warn(\"Variable '\" + varname + \"' has dimensions + '\" + str(da.dims) + \"' which are not considered for spatial aggregation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_list = list(dataset['component'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessTimeSeries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_dict = vars_ts\n",
    "n_regions = len(dataset['space'].values)\n",
    "n_components = len(component_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocesses data array correspinding to each time series variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var, da in vars_dict.items():\n",
    "    print(f'var is {var} and data is {da}')\n",
    "    var = var \n",
    "    da = da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_var = np.array([np.zeros(n_regions)]).T\n",
    "print(f'matrix_var is {matrix_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1. Find the valid components for each variable ( valid_component_weight=1, otherwise=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var_mean_df = da.mean(dim=\"space\").mean(dim=\"TimeStep\").to_dataframe()\n",
    "print(f'var_mean_df is {var_mean_df}')\n",
    "    \n",
    "var_mean_df['component_id'] = np.array(range(n_components))\n",
    "print(f'var_mean_df is {var_mean_df}')\n",
    "    \n",
    "valid_component_ids = list(var_mean_df[var_mean_df[var].notna()]['component_id'])\n",
    "print(f'valid_component_ids is {valid_component_ids}') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2. Preprocess data corresponding to each valid component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for comp_id in valid_component_ids:\n",
    "    print(f'da[{comp_id}].values is {da[comp_id].values}')\n",
    "    # Compute the standardized matrix for each valid component: rescale the matrix value to range [0,1]\n",
    "    # -> the values in time series for this component should be in the same scaling: matrix_MinMaxScaler()\n",
    "    #### STEP 2a. Obtain a scaled matirx for each valid component's matrix \n",
    "    matrix_var_c = grouping_utils.matrix_MinMaxScaler(da[comp_id].values) \n",
    "    print(f'matrix_var_c is {matrix_var_c}')  \n",
    "    \n",
    "    #### STEP 2b. Join this matrix to the resultant matrix (column-wise) -> matrix of component1 | matrix of component2 \n",
    "    # Concatenate this matrix block of one component to the final matrix for this 2d variable\n",
    "    matrix_var = np.concatenate((matrix_var, matrix_var_c), axis=1)\n",
    "    \n",
    "matrix_var = np.delete(matrix_var,0,1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3. Add it to the resultant dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ts[var] = matrix_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each variable has a matrix value\n",
    "for var, da in vars_dict.items():\n",
    "\n",
    "    matrix_var = np.array([np.zeros(n_regions)]).T\n",
    "\n",
    "    # Find the valid components for each variable: valid_component_weight=1, otherwise=0\n",
    "    var_mean_df = da.mean(dim=\"space\").mean(dim=\"TimeStep\").to_dataframe()\n",
    "    var_mean_df['component_id'] = np.array(range(n_components))\n",
    "    valid_component_ids = list(var_mean_df[var_mean_df[var].notna()]['component_id'])\n",
    "\n",
    "    for comp_id in valid_component_ids:\n",
    "        # Compute the standardized matrix for each valid component: rescale the matrix value to range [0,1]\n",
    "        # -> the values in time series for this component should be in the same scaling: matrix_MinMaxScaler()\n",
    "        matrix_var_c = grouping_utils.matrix_MinMaxScaler(da[comp_id].values) \n",
    "\n",
    "        # Concatenate this matrix block of one component to the final matrix for this 2d variable\n",
    "        matrix_var = np.concatenate((matrix_var, matrix_var_c), axis=1)\n",
    "\n",
    "    matrix_var = np.delete(matrix_var,0,1)\n",
    "\n",
    "    ds_ts[var] = matrix_var\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matrix_MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = da[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max, x_min = 1,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((X - np.min(X)) / (np.max(X) - np.min(X))) * (x_max - x_min) + x_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess1dVariables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_dict = vars_1d\n",
    "n_components = len(component_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = prep.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocesses data array correspinding to each 1d variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var, da in vars_dict.items():\n",
    "    var = var\n",
    "    da = da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1. Find the valid components for each variable ( valid_component_weight=1, otherwise=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var_mean_df = da.mean(dim=\"space\").to_dataframe()\n",
    "var_mean_df['component_id'] = np.array(range(n_components))\n",
    "valid_component_ids = list(var_mean_df[var_mean_df[var].notna()]['component_id'])\n",
    "valid_component_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2. Retain only the valid components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = da.values[valid_component_ids]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3. Scale, transform, and add it to the resultant dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1d[var] = min_max_scaler.fit_transform(data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess2dVariables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_dict = vars_2d\n",
    "component_list = component_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2d_distance': <xarray.DataArray '2d_distance' (component: 4, space: 3, space_2: 3)>\n",
       " array([[[ 0. ,  1. ,  2. ],\n",
       "         [ 1. ,  0. , 10. ],\n",
       "         [ 2. , 10. ,  0. ]],\n",
       " \n",
       "        [[ 0. ,  0.1,  0.2],\n",
       "         [ 0.1,  0. ,  1. ],\n",
       "         [ 0.2,  1. ,  0. ]],\n",
       " \n",
       "        [[ nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan]],\n",
       " \n",
       "        [[ nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan]]])\n",
       " Coordinates:\n",
       "   * component  (component) <U2 'c1' 'c2' 'c3' 'c4'\n",
       "   * space      (space) <U6 '01_reg' '02_reg' '03_reg'\n",
       "   * space_2    (space_2) <U6 '01_reg' '02_reg' '03_reg'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = len(component_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2d = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocesses data array correspinding to each 2d variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var, da in vars_dict.items():\n",
    "    var = var \n",
    "    da = da\n",
    "    ds_2d_var = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different region orders\n",
    "space1 = da.space.values\n",
    "space2 = da.space_2.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.  Find the valid components for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "var_mean_df = da.mean(dim=\"space\").mean(dim=\"space_2\").to_dataframe()\n",
    "var_mean_df['component_id'] = np.array(range(n_components))\n",
    "valid_component_ids = list(var_mean_df[var_mean_df[var].notna()]['component_id'])\n",
    "valid_component_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2. # For each valid component : obtain hollow and symmetric connectivity matrix, scale the matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.]\n",
      " [ 1.  0. 10.]\n",
      " [ 2. 10.  0.]]\n",
      "-----------------------------------------------------------------------------------\n",
      "   01_reg  02_reg  03_reg\n",
      "0     0.0     1.0     2.0\n",
      "1     1.0     0.0    10.0\n",
      "2     2.0    10.0     0.0\n",
      "-----------------------------------------------------------------------------------\n",
      "   01_reg  02_reg  03_reg\n",
      "0     0.0     1.0     2.0\n",
      "1     1.0     0.0    10.0\n",
      "2     2.0    10.0     0.0\n",
      "-----------------------------------------------------------------------------------\n",
      "[[0.  0.1 0.2]\n",
      " [0.1 0.  1. ]\n",
      " [0.2 1.  0. ]]\n",
      "-----------------------------------------------------------------------------------\n",
      "[[0.  0.1 0.2]\n",
      " [0.1 0.  1. ]\n",
      " [0.2 1.  0. ]]\n",
      "-----------------------------------------------------------------------------------\n",
      "   01_reg  02_reg  03_reg\n",
      "0     0.0     0.1     0.2\n",
      "1     0.1     0.0     1.0\n",
      "2     0.2     1.0     0.0\n",
      "-----------------------------------------------------------------------------------\n",
      "   01_reg  02_reg  03_reg\n",
      "0     0.0     0.1     0.2\n",
      "1     0.1     0.0     1.0\n",
      "2     0.2     1.0     0.0\n",
      "-----------------------------------------------------------------------------------\n",
      "[[0.  0.1 0.2]\n",
      " [0.1 0.  1. ]\n",
      " [0.2 1.  0. ]]\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for comp_id in valid_component_ids:\n",
    "            \n",
    "    \n",
    "    var_matr = da[comp_id].values\n",
    "    print(var_matr)\n",
    "    print('-----------------------------------------------------------------------------------')        \n",
    "    #### STEP 2a.obtain hollow and symmetric dist matrix -> order of space and space2 is the same \n",
    "    da_comp_df = pd.DataFrame(data=var_matr,columns=space2)\n",
    "    print(da_comp_df)\n",
    "    print('-----------------------------------------------------------------------------------') \n",
    "    da_comp_df = da_comp_df[space1]\n",
    "    print(da_comp_df)\n",
    "    print('-----------------------------------------------------------------------------------')   \n",
    "    #### STEP 2b.  scale the matrix \n",
    "    # Standardize the matrix: keep all the values non-negative! AND keep zeros to be zeros (not change the meaning of connectivity!)\n",
    "    # => scale the data to the range [0,1]\n",
    "    ds_2d_var[comp_id] = grouping_utils.matrix_MinMaxScaler(da_comp_df.to_numpy())\n",
    "    print(ds_2d_var[comp_id])\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "ds_2d[var] = ds_2d_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2d_distance': {0: array([[0. , 0.1, 0.2],\n",
       "         [0.1, 0. , 1. ],\n",
       "         [0.2, 1. , 0. ]]),\n",
       "  1: array([[0. , 0.1, 0.2],\n",
       "         [0.1, 0. , 1. ],\n",
       "         [0.2, 1. , 0. ]])}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3. Perform only if handle_mode='toDissimilarity': condense the matrix, convert from similarity to dissimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d_distance\n",
      "{0: array([[0. , 0.1, 0.2],\n",
      "       [0.1, 0. , 1. ],\n",
      "       [0.2, 1. , 0. ]]), 1: array([[0. , 0.1, 0.2],\n",
      "       [0.1, 0. , 1. ],\n",
      "       [0.2, 1. , 0. ]])}\n",
      "-----------------------------------------------------------------------------------\n",
      "0\n",
      "[[0.  0.1 0.2]\n",
      " [0.1 0.  1. ]\n",
      " [0.2 1.  0. ]]\n",
      "-----------------------------------------------------------------------------------\n",
      "[0.1 0.2 1. ]\n",
      "-----------------------------------------------------------------------------------\n",
      "[0.9 0.8 0. ]\n",
      "-----------------------------------------------------------------------------------\n",
      "{'2d_distance': {0: array([0.9, 0.8, 0. ]), 1: array([[0. , 0.1, 0.2],\n",
      "       [0.1, 0. , 1. ],\n",
      "       [0.2, 1. , 0. ]])}}\n",
      "-----------------------------------------------------------------------------------\n",
      "1\n",
      "[[0.  0.1 0.2]\n",
      " [0.1 0.  1. ]\n",
      " [0.2 1.  0. ]]\n",
      "-----------------------------------------------------------------------------------\n",
      "[0.1 0.2 1. ]\n",
      "-----------------------------------------------------------------------------------\n",
      "[0.9 0.8 0. ]\n",
      "-----------------------------------------------------------------------------------\n",
      "{'2d_distance': {0: array([0.9, 0.8, 0. ]), 1: array([0.9, 0.8, 0. ])}}\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for var, var_dict in ds_2d.items():\n",
    "    print(var)\n",
    "    print(var_dict)\n",
    "    print('-----------------------------------------------------------------------------------')        \n",
    "    # Transform the symmetric connectivity matrix to 1-dim distance vector\n",
    "    for c, data in var_dict.items():\n",
    "        print(c) \n",
    "        print(data)\n",
    "        print('-----------------------------------------------------------------------------------')\n",
    "        #### STEP 3a. condense the matrix  \n",
    "        # Obtain the vector form of this symmetric connectivity matrix, in the range [0,1]\n",
    "        # Deactivate checks since small numerical errors can be in the dataset\n",
    "        vec = hierarchy.distance.squareform(data, checks=False)\n",
    "        print(vec)\n",
    "        print('-----------------------------------------------------------------------------------')\n",
    "        #### STEP 3b.Convert the value of connectivity (similarity) to distance (dissimilarity)\n",
    "        vec = 1 - vec\n",
    "        print(vec)\n",
    "        print('-----------------------------------------------------------------------------------')        \n",
    "        # Distance vector for this 2d variable and this component: 1 means maximum distance!\n",
    "        ds_2d[var][c] = vec\n",
    "        print(ds_2d)\n",
    "        print('-----------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2d_distance': {0: array([0.9, 0.8, 0. ]), 1: array([0.9, 0.8, 0. ])}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if handle_mode='toAffinity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if handle_mode == 'toAffinity':\n",
    "        '''Original matrices as Adjacency matrices : \n",
    "            - adjacency matrix: 0 means identical elements; high values means very similar elements\n",
    "            - adjacency matrix of a graph: symmetric, diagonals = 0\n",
    "            - add all matrices of different components for each variable \n",
    "            \n",
    "        '''\n",
    "        return ds_2d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
