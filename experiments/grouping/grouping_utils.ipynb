{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import warnings\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn import metrics\n",
    "\n",
    "import FINE.spagat.dataset as spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_MinMaxScaler(X, x_min=0, x_max=1):\n",
    "    ''' Standardize a numpy matrix to range [0,1], NOT column-wise, but matrix-wise!\n",
    "    '''\n",
    "    if np.max(X) == np.min(X): \n",
    "        return X\n",
    "    return ((X - np.min(X)) / (np.max(X) - np.min(X))) * (x_max - x_min) + x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTimeSeries(vars_dict, n_regions, n_components):\n",
    "    '''Preprocess the input dictionary of time series variables\n",
    "        - Input vars_dic: dimensions of each variable value are 'component','space','TimeStep'\n",
    "        - Output ds_ts: a dictionary containing all 2d variables\n",
    "            - For each variable: the value is a flattened data feature matrix based on its valid components\n",
    "                - size: Row (n_regions) * Column (n_components * n_timesteps)\n",
    "            - matrix block for each valid component of one particular variable: \n",
    "                - the value is a numpy array of size n_regions * TimeStep         \n",
    "                - the array matrix is normalized to scale [0,1]     \n",
    "    '''\n",
    "    if not vars_dict: return None\n",
    "\n",
    "    ds_ts = {}\n",
    "\n",
    "    # Each variable has a matrix value\n",
    "    for var, da in vars_dict.items():\n",
    "\n",
    "        matrix_var = np.array([np.zeros(n_regions)]).T\n",
    "\n",
    "        # Find the valid components for each variable: valid_component_weight=1, otherwise=0\n",
    "        var_mean_df = da.mean(dim=\"space\").mean(dim=\"TimeStep\").to_dataframe()\n",
    "        var_mean_df['component_id'] = np.array(range(n_components))\n",
    "        valid_component_ids = list(var_mean_df[var_mean_df[var].notna()]['component_id'])\n",
    "\n",
    "        for comp_id in valid_component_ids:\n",
    "            # Compute the standardized matrix for each valid component: rescale the matrix value to range [0,1]\n",
    "            # -> the values in time series for this component should be in the same scaling: matrix_MinMaxScaler()\n",
    "            matrix_var_c = matrix_MinMaxScaler(da[comp_id].values) \n",
    "\n",
    "            # Concatenate this matrix block of one component to the final matrix for this 2d variable\n",
    "            matrix_var = np.concatenate((matrix_var, matrix_var_c), axis=1)\n",
    "\n",
    "        matrix_var = np.delete(matrix_var,0,1)\n",
    "\n",
    "        ds_ts[var] = matrix_var\n",
    "           \n",
    "    return ds_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess1dVariables(vars_dict, n_components):\n",
    "    ''' Preprocess 1-dimensional variables\n",
    "        - return a dictionary containing a numpy matrix for all 1d variables\n",
    "        - each value is a numpy array of size n_regions * n_valid_components_of_each_variable, \n",
    "            e.g. 96*6 for '1d_capacityFix'\n",
    "        - the numpy arrays are standardized, rescaling to the range [0,1] in column-wise, i.e. rescaling for each component\n",
    "    '''\n",
    "\n",
    "    if not vars_dict: return None\n",
    "\n",
    "    ds_1d = {}\n",
    "\n",
    "    min_max_scaler = prep.MinMaxScaler()\n",
    "\n",
    "    for var, da in vars_dict.items():\n",
    "        \n",
    "        # Find the valid components for each variable: valid_comp_weight=1, otherwise=0\n",
    "        var_mean_df = da.mean(dim=\"space\").to_dataframe()\n",
    "        var_mean_df['component_id'] = np.array(range(n_components))\n",
    "        valid_component_ids = list(var_mean_df[var_mean_df[var].notna()]['component_id'])\n",
    "        \n",
    "\n",
    "        # Retain only the valid components\n",
    "        data = da.values[valid_component_ids]\n",
    "        ds_1d[var] = min_max_scaler.fit_transform(data.T)\n",
    "\n",
    "    return ds_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2dVariables(vars_dict, component_list, handle_mode='toDissimilarity'):\n",
    "    ''' Preprocess matrices of 2d-vars with one of the following mode:\n",
    "        - Firstly: Adjust the region order of space_2, i.e. order of columns\n",
    "        - Obtain ds_2d: a dictionary containing all variables\n",
    "            - For each variable: the value is a dictionary containing all its valid components\n",
    "            - For each variable and each valid component: \n",
    "                - the value is a numpy array of size n_regions*n_regions\n",
    "                - the matrix is symmetrical. (undirected graph), with zero diagonal values\n",
    "                - all the values in the matrices are NON-Negative!\n",
    "\n",
    "        - Return: a dictionary containing a matrix / vector for each 2d var and each component\n",
    "            - standardize the vector (rescaling!)\n",
    "            - Possible TO-DO: add the matrics together?\n",
    "\n",
    "        How to handle the matrices:\n",
    "            - handle_mode == 'toDissimilarity': Convert matrices to a distance matrix by transforming the connectivity values to distance meaning\n",
    "            - handle_mode='toAffinity': extract the matrices of all variables and add them up as one adjacency matrix for spectral clustering\n",
    "    '''\n",
    "\n",
    "    if not vars_dict: return None\n",
    "\n",
    "    n_components = len(component_list)\n",
    "\n",
    "    # Obtain th dictionary of connectivity matrices for each variable and for its valid component, each of size n_regions*n_regions (n_regions)\n",
    "    ds_2d = {}\n",
    "\n",
    "    for var, da in vars_dict.items():\n",
    "        \n",
    "        ds_2d_var = {}\n",
    "        \n",
    "        # Different region orders\n",
    "        space1 = da.space.values\n",
    "        space2 = da.space_2.values\n",
    "        \n",
    "        # Find the valid components for each variable\n",
    "        var_mean_df = da.mean(dim=\"space\").mean(dim=\"space_2\").to_dataframe()\n",
    "        var_mean_df['component_id'] = np.array(range(n_components))\n",
    "        valid_component_ids = list(var_mean_df[var_mean_df[var].notna()]['component_id'])\n",
    "        \n",
    "        # DataArray da for this 2d-variable, component * space * space_2\n",
    "        for comp_id in valid_component_ids:\n",
    "            \n",
    "            # Under each component, there is a (e.g. 96*96) squares matrix to represent the connection features\n",
    "            var_matr = da[comp_id].values\n",
    "            \n",
    "            # Rearrange the columns order --> the regions order of space_2!\n",
    "            da_comp_df = pd.DataFrame(data=var_matr,columns=space2)\n",
    "            da_comp_df = da_comp_df[space1]\n",
    "            \n",
    "            # Standardize the matrix: keep all the values non-negative! AND keep zeros to be zeros (not change the meaning of connectivity!)\n",
    "            # => scale the data to the range [0,1]\n",
    "            ds_2d_var[comp_id] = matrix_MinMaxScaler(da_comp_df.to_numpy())\n",
    "        \n",
    "        ds_2d[var] = ds_2d_var\n",
    "\n",
    "    # Handle the matrices according to clustering methods\n",
    "\n",
    "    ## Possible TO-DO: maybe other transformation methods are possible, e.g. Gaussian (RBF, heat) kernel\n",
    "    if handle_mode == 'toDissimilarity':\n",
    "        ''' Convert similarity matrix (original symmetric connectivity matrix) into dissimilarity matrix (1-dim Distance vector)\n",
    "            - higher values for connectivity means: the two regions are more likely to be grouped -> can be regarded as smaller distance!\n",
    "            \n",
    "            - Rescaling the connectivity matrix to range [0,1], where 1 means maximum similarity\n",
    "            - dissim(x) = 1 - sim(x)\n",
    "        '''\n",
    "\n",
    "        # Obtain a dictionary containing one distance vector (1-dim matrix) for each variable and for each valid component\n",
    " \n",
    "        min_max_scaler = prep.MinMaxScaler() #TODO: remove the (unused) line\n",
    "\n",
    "        for var, var_dict in ds_2d.items():\n",
    "            \n",
    "            # Transform the symmetric connectivity matrix to 1-dim distance vector\n",
    "            for c, data in var_dict.items():\n",
    "                \n",
    "                # Obtain the vector form of this symmetric connectivity matrix, in the range [0,1]\n",
    "                # Deactivate checks since small numerical errors can be in the dataset\n",
    "                vec = hierarchy.distance.squareform(data, checks=False)\n",
    "\n",
    "                # Convert the value of connectivity (similarity) to distance (dissimilarity)\n",
    "                vec = 1 - vec\n",
    "                \n",
    "                # Distance vector for this 2d variable and this component: 1 means maximum distance!\n",
    "                ds_2d[var][c] = vec\n",
    "  \n",
    "        return ds_2d\n",
    "\n",
    "    if handle_mode == 'toAffinity':\n",
    "        '''Original matrices as Adjacency matrices : \n",
    "            - adjacency matrix: 0 means identical elements; high values means very similar elements\n",
    "            - adjacency matrix of a graph: symmetric, diagonals = 0\n",
    "            - add all matrices of different components for each variable \n",
    "            \n",
    "        '''\n",
    "        return ds_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessDataset(sds, handle_mode, vars='all', dims='all', var_weightings=None):\n",
    "    '''Preprocess the Xarray dataset: Separate the dataset into 3 parts: time series data, 1d-var data, and 2d-var data\n",
    "        - vars_ts: Time series variables: a feature matrix for each ts variable\n",
    "        - vars_1d: a features matrix for each 1d variable\n",
    "        - vars_2d: 2d variables showing connectivity between regions\n",
    "            - Hierarchical: directly transformed as distance matrix (values showing dissimilarity) and combine with vars_ts and vars_1d\n",
    "            - Spectral: extract this part as an affinity matrix for the (un)directed graph (indicating similarity, here using adjacency matrix)\n",
    "            - handle_mode: decide which method to preprocess vars_2d\n",
    "\n",
    "        - Return: the three parts separately \n",
    "    '''\n",
    "\n",
    "    dataset = sds.xr_dataset\n",
    "\n",
    "    # Traverse all variables in the dataset, and put them in separate categories\n",
    "    vars_ts = {}\n",
    "    vars_1d = {}\n",
    "    vars_2d = {}\n",
    "\n",
    "    for varname, da in dataset.data_vars.items():\n",
    "        # sort the dimensions\n",
    "        if sorted(da.dims) == sorted(('component','Period','TimeStep', 'space')):   #TODO: maybe space should be generalized with additional variable - dimension_description ?\n",
    "            # Period is not considered -> TODO: consider the Period dimension.\n",
    "            da = da.transpose('Period','component','space','TimeStep')[0]  \n",
    "            vars_ts[varname] = da\n",
    "\n",
    "        elif sorted(da.dims) == sorted(('component','space')):\n",
    "            vars_1d[varname] = da\n",
    "\n",
    "        elif sorted(da.dims) == sorted(('component','space','space_2')):\n",
    "            vars_2d[varname] = da\n",
    "\n",
    "        else:\n",
    "            warnings.warn(\"Variable '\" + varname + \"' has dimensions + '\" + str(da.dims) + \"' which are not considered for spatial aggregation.\")\n",
    "\n",
    "    component_list = list(dataset['component'].values)\n",
    "\n",
    "    n_regions = len(dataset['space'].values)\n",
    "\n",
    "    ds_timeseries = preprocessTimeSeries(vars_ts, n_regions, len(component_list))\n",
    "    ds_1d_vars = preprocess1dVariables(vars_1d, len(component_list))\n",
    "    \n",
    "    if handle_mode == 'toDissimilarity':\n",
    "        ''' Return 3 (all standardized with minMaxScaler) dictionaries for each variable category:\n",
    "            - timeseries vars: a dictionary containing one feature matrix for each variable\n",
    "            - 1d vars: a dictionary containing one feature matrix for each variable\n",
    "            - 2d vars: vectors for each variable and for each valid component  \n",
    "                - 1-dim vector indicating dissimilarities between two regions (distance) \n",
    "                - vector length = n_regs * (n_regs - 1) / 2\n",
    "        '''\n",
    "        ds_2d_vars = preprocess2dVariables(vars_2d, component_list, handle_mode='toDissimilarity')\n",
    "\n",
    "        return ds_timeseries, ds_1d_vars, ds_2d_vars\n",
    "\n",
    "    #TODO: try negative var_weights for some 2d vars \n",
    "    if handle_mode == 'toAffinity':\n",
    "        ''' Return 3 affinity matrices:\n",
    "            - timeseries: one data feature matrix\n",
    "                - concatenate matrices for different variables with weighting factors\n",
    "                - return one matrix of size: n_regions * columns=(n_ts_vars * n_valid_component_per_var * n_timesteps)\n",
    "            - 1d vars: one data feature matrix\n",
    "                - concatenate matrices of various vars to one matrix with weighting factor for each var\n",
    "                - return the matrix of size: n_regions * columns =(n_1d_vars * n_valid_component_per_var) \n",
    "            - 2d vars: one single adjacency matrix\n",
    "                - original matrices as adjacency matrices\n",
    "                - add them to one single matrix with weighting factors for each var\n",
    "                - from adjacency matrix to affinity matrix\n",
    "        '''\n",
    "        # Weighting factors of each variable \n",
    "        if var_weightings:\n",
    "            var_weightings = var_weightings               #TODO: reduce the lines here by using 'if var_weightings is None' \n",
    "        else:\n",
    "            vars_list = list(vars_ts.keys()) + list(vars_1d.keys()) + list(vars_2d.keys())\n",
    "            var_weightings = dict.fromkeys(vars_list,1)\n",
    "\n",
    "        ###### For Time series vars: obtain the single matrix - matrix_ts\n",
    "        matrix_ts = np.array([np.zeros(n_regions)]).T\n",
    "\n",
    "        n_timesteps = len(dataset['TimeStep'].values)\n",
    "\n",
    "        for var, var_matrix in ds_timeseries.items():\n",
    "\n",
    "            weight = var_weightings[var]\n",
    "            \n",
    "            # Concatenate the matrix of this var to the final matrix with its weighting factor\n",
    "            matrix_ts = np.concatenate((matrix_ts, var_matrix * weight), axis=1)\n",
    "        \n",
    "        matrix_ts = np.delete(matrix_ts,0,1)\n",
    "\n",
    "        ###### For 1d vars: obtain the single matrix - matrix_1d\n",
    "        matrix_1d = np.array([np.zeros(n_regions)]).T\n",
    "\n",
    "        for var, var_matrix in ds_1d_vars.items():\n",
    "\n",
    "            weight = var_weightings[var]\n",
    "\n",
    "            # Concatenate the matrix of this vars to one single 1d matrix with weight factor\n",
    "            matrix_1d = np.concatenate((matrix_1d, var_matrix * weight),axis=1)\n",
    "        \n",
    "        matrix_1d = np.delete(matrix_1d,0,1)\n",
    "\n",
    "        ###### For 2d vars: obtain a single square matrix of size n_regions*regions\n",
    "        matrix_2d = np.zeros((n_regions,n_regions))\n",
    "\n",
    "        ds_2d_vars = preprocess2dVariables(vars_2d, component_list, handle_mode='toAffinity')\n",
    "\n",
    "        # After adding, the value in matrix_2d is not in the range [0,1] any more\n",
    "        for var, var_dict in ds_2d_vars.items():\n",
    "\n",
    "            weight = var_weightings[var]\n",
    "\n",
    "            # Add the matrices of different components for one var to a single matrix\n",
    "            for component, data in var_dict.items():\n",
    "                matrix_2d += data * weight\n",
    "\n",
    "        ###### Return 3 separate matrices\n",
    "        return matrix_ts, matrix_1d, matrix_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfDistance(ds_ts, ds_1d, ds_2d, n_regions, a, b, var_weightings=None, part_weightings=None):  #TODO: Change a and b to something more intuitive \n",
    "    ''' Custom distance function: \n",
    "        - parameters a, b: region ids, a < b, a,b in [0, n_regions) \n",
    "        - return: distance between a and b = distance_ts + distance_1d + distance_2d\n",
    "            - distance for time series: ** dist_var_component_timestep ** -> value subtraction\n",
    "            - distance for 1d vars: sum of (value subtraction in ** dist_var_component ** )\n",
    "            - distance for 2d vars: corresponding value in ** dist_var_component **\n",
    "            - each partial distance need to be divided by sum of variable weight factors, \n",
    "                i.e. number of variables when all weight factors are 1, \n",
    "                to reduce the effect of variables numbers on the final distance.\n",
    "        ---\n",
    "        Metric space properties :  -> at the same level of data structure! in the same distance space!\n",
    "        - Non-negativity: d(i,j) > 0\n",
    "        - Identity of indiscernibles: d(i,i) = 0   ---> diagonal must be 0!\n",
    "        - Symmetry: d(i,j) = d(j,i)  ---> the part_2 must be Symmetrical!\n",
    "        - Triangle inequality: d(i,j) <= d(i,k) + d(k,j)  ---> NOT SATISFIED!!!\n",
    "    '''\n",
    "\n",
    "    # Weighting factors of each variable \n",
    "    if var_weightings:                       \n",
    "        var_weightings = var_weightings\n",
    "    else:                                    #TODO: Skip the if statement and \"if not var_weightings\" here\n",
    "        vars_list = list(ds_ts.keys()) + list(ds_1d.keys()) + list(ds_2d.keys())\n",
    "        var_weightings = dict.fromkeys(vars_list,1)\n",
    "\n",
    "    # Weighting factors for 3 var-categories\n",
    "    if part_weightings:\n",
    "        part_weightings = part_weightings\n",
    "    else:                              #TODO: similar to the above change\n",
    "        part_weightings = [1,1,1]\n",
    "\n",
    "    # Distance of Time Series Part\n",
    "    distance_ts = 0\n",
    "    for var, var_matr in ds_ts.items():\n",
    "\n",
    "        var_weight_factor = var_weightings[var]\n",
    "\n",
    "        # Vectors for the two data points (regions), each feature refers to [one valid component & one timestep] for this var\n",
    "        reg_a = var_matr[a]\n",
    "        reg_b = var_matr[b]\n",
    "\n",
    "        # dist_ts(a,b) = sum_var( var_weight * dist_var(a,b) )\n",
    "        # dist_var(a,b) = sum_c(sum_t( [value_var_c_t(a) - value_var_c_t(b)]^2 ))\n",
    "        distance_ts += sum( np.power((reg_a - reg_b),2) ) * var_weight_factor\n",
    "\n",
    "    # Distance of 1d Variables Part\n",
    "    distance_1d = 0\n",
    "    for var, var_matr in ds_1d.items():\n",
    "\n",
    "        var_weight_factor = var_weightings[var]\n",
    "\n",
    "        # Vectors for the two data points (regions), each feature refers to one valid component for this var\n",
    "        reg_a = var_matr[a]\n",
    "        reg_b = var_matr[b]\n",
    "\n",
    "        # dist_1d(a,b) = sum_var{var_weight * sum_c( [value_var_c(a) - value_var_c(b)]^2 ) }\n",
    "        distance_1d += sum(np.power((reg_a - reg_b),2)) * var_weight_factor\n",
    "\n",
    "    # Distance of 2d Variables Part\n",
    "    distance_2d = 0\n",
    "\n",
    "    # The index of corresponding value for region[a] and region[b] in the distance vectors\n",
    "    index_regA_regB = a * (n_regions - a) + (b - a) -1\n",
    "    \n",
    "    for var, var_dict in ds_2d.items():\n",
    "\n",
    "        var_weight_factor = var_weightings[var]\n",
    "\n",
    "        for component, data in var_dict.items():\n",
    "            # Find the corresponding distance value for region_a and region_b \n",
    "            value_var_c = data[index_regA_regB]\n",
    "\n",
    "            if not np.isnan(value_var_c):\n",
    "                # dist_2d(a,b) = sum_var{var_weight * sum_c( [value_var_c(a,b)]^2 ) }\n",
    "                distance_2d += (value_var_c*value_var_c) * var_weight_factor\n",
    "\n",
    "    return distance_ts * part_weightings[0] + distance_1d * part_weightings[1] + distance_2d * part_weightings[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfDistanceMatrix(ds_ts, ds_1d, ds_2d, n_regions, var_weightings=None):\n",
    "    ''' Return a n_regions by n_regions symmetric distance matrix X \n",
    "    '''\n",
    "\n",
    "    distMatrix = np.zeros((n_regions,n_regions))\n",
    "\n",
    "    for i in range(n_regions):\n",
    "        for j in range(i+1,n_regions):\n",
    "            distMatrix[i,j] = selfDistance(ds_ts,ds_1d, ds_2d, n_regions, i,j, var_weightings=var_weightings)\n",
    "\n",
    "    distMatrix += distMatrix.T - np.diag(distMatrix.diagonal())\n",
    "\n",
    "    return distMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateConnectivityMatrix(sds):\n",
    "    ''' Generate an adjacency matrix to show the neighboring structure\n",
    "        - For every index pair of regions, as long as they have a non-zero 2d-variable value, related to pipeline component, they are regarded as connected.\n",
    "        - 1 means connected, otherwise 0\n",
    "        - If no component related to pipeline, then consider all other components\n",
    "    '''\n",
    "    ds_extracted = sds.xr_dataset\n",
    "\n",
    "    vars_2d = {}\n",
    "\n",
    "    for varname, da in ds_extracted.data_vars.items():\n",
    "        if da.dims == ('component','space','space_2'):\n",
    "            vars_2d[varname] = da\n",
    "\n",
    "    n_regions = len(ds_extracted['space'].values)\n",
    "    component_list = list(ds_extracted['component'].values)\n",
    "    n_components = len(component_list)\n",
    "\n",
    "    # Square matrices for each 2d variable and each valid component\n",
    "    ds_2d = preprocess2dVariables(vars_2d, component_list, handle_mode='toAffinity')\n",
    "\n",
    "    # The neighboring information is based on the 2d vars with components related to pipeline\n",
    "    connect_components = []\n",
    "    for i in range(len(component_list)):\n",
    "        if 'pipeline' in component_list[i].lower():\n",
    "            connect_components.append(i)\n",
    "\n",
    "    # If there is no components related to pipelines, then consider all existing components.\n",
    "    if not connect_components:\n",
    "        connect_components = list(range(len(component_list)))\n",
    "\n",
    "    adjacencyMatrix = np.zeros((n_regions,n_regions))\n",
    "\n",
    "    # Check each index pair of regions to verify, if the two regions are connected to each other\n",
    "    for i in range(n_regions):\n",
    "        for j in range(i+1,n_regions):\n",
    "            if checkConnectivity(i,j, ds_2d, connect_components):\n",
    "                adjacencyMatrix[i,j] = 1\n",
    "\n",
    "    adjacencyMatrix += adjacencyMatrix.T - np.diag(adjacencyMatrix.diagonal())\n",
    "\n",
    "    # Set the diagonal values as 1\n",
    "    np.fill_diagonal(adjacencyMatrix, 1)\n",
    "\n",
    "    return adjacencyMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkConnectivity(i,j, ds_2d, connect_components):\n",
    "    '''Check if region i is neighboring to region j, based on the components related to pipelines.\n",
    "        - as 1 if there exists at least one non-zero value in any matrix at the position [i,j]\n",
    "        - if no components related to pipelines, then the connect_components is the list of all existing components.\n",
    "    '''\n",
    "    \n",
    "    for var, var_dict in ds_2d.items():\n",
    "        for c, data in var_dict.items():\n",
    "            if (c in connect_components) and (data[i,j] != 0):\n",
    "                return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeModularity(adjacency, regions_label_list):\n",
    "    ''' Compute the modularity of the partitioned graph\n",
    "        - graph's weighted adjacency matrix with entries defined by the edge weights\n",
    "        - regions_label_list to show the graph partition\n",
    "    '''\n",
    "\n",
    "    np.fill_diagonal(adjacency, 0)\n",
    "    n_regions = len(regions_label_list)\n",
    "\n",
    "    # Values in the adjacency matrix as edge weights\n",
    "    edge_weights_sum = np.sum(adjacency)\n",
    "\n",
    "    modularity = 0\n",
    "\n",
    "    for v in range(n_regions):\n",
    "        for w in range(v+1, n_regions):\n",
    "\n",
    "            # The weighted degree of nodes: sum of node's incident edge weights\n",
    "            d_v = np.sum(adjacency[v])\n",
    "            d_w = np.sum(adjacency[w])\n",
    "\n",
    "            # If the two nodes belong to the same cluster\n",
    "            delta = 1 if regions_label_list[v] == regions_label_list[w] else 0\n",
    "\n",
    "            # Sum up the actual fraction of the edges minus the expected fraction of edges inside of each cluster\n",
    "            modularity += (adjacency[v,w] - (d_v * d_w) / (2 * edge_weights_sum)) * delta\n",
    "\n",
    "    modularity = modularity / (2 * edge_weights_sum)\n",
    "\n",
    "    return modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSilhouetteCoefficient(regions_list, distanceMatrix, aggregation_dict):\n",
    "\n",
    "    n_regions = len(regions_list)\n",
    "\n",
    "    # Silhouette Coefficient scores\n",
    "    scores = [0 for i in range(1, n_regions-1)]\n",
    "\n",
    "    # Labels for each region object\n",
    "    labels = [0 for i in range(n_regions)]\n",
    "\n",
    "    for k, regions_dict in aggregation_dict.items():\n",
    "\n",
    "        if k == 1 or k == n_regions:\n",
    "            continue\n",
    "\n",
    "        # Obtain labels list for this clustering results\n",
    "        label = 0\n",
    "        for sup_region in regions_dict.values():\n",
    "            for reg in sup_region:\n",
    "                ind = regions_list.index(reg)\n",
    "                labels[ind] = label\n",
    "            \n",
    "            label += 1\n",
    "        \n",
    "        # Silhouette score of this clustering\n",
    "        s = metrics.silhouette_score(distanceMatrix, labels, metric='precomputed')\n",
    "        scores[k-2] = s\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
